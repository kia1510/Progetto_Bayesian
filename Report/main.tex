\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{titling}
\usepackage[numbers,super]{natbib}
\usepackage{xcolor} % Pacchetto per i colori

\title{\textcolor{red}{Differential Privacy}}
\author{Giovanni Mele, Chiara Noemi Nesti, Chiara Tomasini,\\
Gianluca Villa, Andrea Violante, Stefano Zara}
\date{} % La data è vuota

\begin{document}

\maketitle % Aggiungi questo comando per generare il titolo

\begin{flushright}
\textit{Supervised by:}\\
Prof. Mario Beraha\\
PhD researcher at Politecnico di Milano
\end{flushright}


% Nuova pagina per l'indice
\newpage
\tableofcontents

% Inizio del contenuto
\newpage

\section{\textcolor{red}{Abstract}}
Nowadays, data play an increasingly central role in society; a substantial portion of these are sensitive data, and their direct use could lead to privacy issues. Nevertheless, utilizing these data is essential for gaining deep insights and making predictions about social and scientific phenomena. A commonly used solution in statistical applications is to generate synthetic data that preserve the properties of the original data while safeguarding privacy.

Our project is based on nonparametric Bayesian statistical (BNP) methods, such as data simulation from Polya Tree processes.

\section{\textcolor{red}{Theoretical Background\textsuperscript{\cite{libro}}}}
\subsection{Dirichlet process}

Nonparametric Bayesian models (BNP) are widely used in statistics for estimating the density of an unknown distribution \( G \). A distinctive feature of the Dirichlet Process (DP) is its capability to represent \( G \) as a weighted combination of point masses.
\medskip

A DP with parameters \( M \) and \( G_0 \) is defined on a probability space \( S \). In this framework, \( M \) is the concentration parameter, while \( G_0 \) serves as the base measure, acting as the ``center'' of the distribution generated by the process.

\medskip
Respect to the parametric setting where the Dirichlet process (DP) is based on the inference about an unknown distribution \( G \) on the basis of an observed i.i.d. sample
\[
y_i \mid G \sim \text{iid } G, \quad i = 1, \ldots, n
\]
If we wish to proceed with Bayesian inference, we need to complete the model with a prior probability model for the unknown parameter \( G \). Assuming a prior model on \( G \) requires the specification of a probability model for an infinite-dimensional parameter, that is, a Bayesian Non Parametric prior.
\bigskip



\textbf{Definition (Dirichlet process):} Let \( M > 0 \) and \( G_0 \) be a probability measure defined on \( S \). A DP with parameters \( (M, G_0) \) is a random probability measure \( G \) defined on \( S \) which assigns probability \( G(B) \) to every (measurable) set \( B \) such that for each (measurable) finite partition \( \{B_1, \ldots, B_k\} \) of \( S \), the joint distribution of the vector \( (G(B_1), \ldots, G(B_k)) \) is the Dirichlet distribution with parameters \( (M G_0(B_1), \ldots, M G_0(B_k)) \).

The distribution can be expressed by
\[
G \mid M, G_0 \sim \text{DP}(M G_0)
\]

\medskip
One important property of the DP is its large support, which allows \( G \) to approximate any probability measure with the same support as \( G_0 \). The proximity of \( G \) to \( G_0 \) is regulated by the parameter \( M \); as \( M \) increases, \( G \) becomes increasingly concentrated around \( G_0 \).

\bigskip
\bigskip

\subsection{Polya Tree}
Recall that Dirichlet Process (DP) random probability measures \( G \) are inherently discrete. An elegant alternative that overcomes this limitation is the Polya Tree (PT) prior, which actually includes DP models as a special case. Unlike the DP, however, the PT prior allows for generating continuous distributions with probability one through an appropriate choice of parameters. Conceptually, the PT defines a random histogram.

Consider a histogram with bins defined by a partition of the sample space into non-empty subsets, denoted as \( \{ B_l, l = 0, \ldots, 2^m - 1 \} \) (where \( 2^m \) represents the partition size, in anticipation of the discussion that follows). For each bin \( B_l \), we can define random probabilities \( G(B_l) \).

Now, refine this histogram by splitting each bin into two sub-bins: \( B_l = B_{l0} \cup B_{l1} \). Then, define random probabilities for the refined histogram by setting \( Y_{l0} = G(B_{l0} \mid B_l) \) and \( Y_{l1} = 1 - Y_{l0} = G(B_{l1} \mid B_l) \). The recursive refinement of bins creates a sequence of nested partitions, which is the fundamental concept behind the PT prior.

\( \Omega = B_0 \cup B_1 \). Thus the subindices of the partitioning subsets are sequences 
\( \epsilon = \epsilon_1 \cdots \epsilon_m \) of binary indicators \( \epsilon_j \in \{0, 1\} \) and 
\( G(B_{\epsilon_1 \cdots \epsilon_m}) = \prod_{j=1}^m Y_{\epsilon_1 \cdots \epsilon_j} \). 
The PT prior is the random probability measure \( G \) that arises when the \( Y_{\epsilon_0} \)'s 
are independent beta random variables.
\bigskip

\textbf{Definition.} Let \(\{B_{\epsilon}\}\), identified with \( \Pi\), be a sequence of nested binary partitions as described before, and let \( A = \{\alpha_{\epsilon} \mid \epsilon \in E \} \) be a collection of nonnegative numbers. A random probability measure \( G \) on \( \Omega \) is said to be a Polya Tree (PT) with parameters \((\Pi\, A)\) if, for every \( m = 1, 2, \ldots \) and every \( \epsilon = \epsilon_1 \ldots \epsilon_m \in E_m \),
\[
G(B_{\epsilon_1 \cdots \epsilon_m}) = \prod_{j=1}^m Y_{\epsilon_1 \cdots \epsilon_j},
\]
where the conditional probabilities \( Y_{\epsilon_1 \cdots \epsilon_{j-1} 0} \) are mutually independent beta random variables, given by
\[
Y_{\epsilon_1 \cdots \epsilon_{j-1} 0} \sim \text{Be}(\alpha_{\epsilon_1 \cdots \epsilon_{j-1} 0}, \alpha_{\epsilon_1 \cdots \epsilon_{j-1} 1}),
\]
and \( Y_{\epsilon_1 \cdots \epsilon_{j-1} 1} = 1 - Y_{\epsilon_1 \cdots \epsilon_{j-1} 0} \). We write \( G \sim \text{PT}(\{B_{\epsilon}\}, A) \).

\bigskip
In practice, it is difficult to elicit the family \( A \) and the partitions \( \Pi \) for a PT model. There are different ways and procedures to solve this problem; we propose mainly two of them:

1. \textbf{Prior Centering by \( \Pi \)} Fix \( \Pi \) as the dyadic quantiles of \( G_0 \) and set all the alphas to be the same at each level.

2. \textbf{Prior Centering by \( A \)}: One can fix \( \Pi \) and select \( A \) such that \( \alpha_{\epsilon_j} \propto G_0(B_{\epsilon_j}) \).

3. \textbf{Canonical Choice for \( \alpha_{\epsilon_1}, \ldots, \alpha_{\epsilon_m} \)}: This last choice is the one we have taken. It is the simplest approach to start with, as we choose alpha at each level as \( \alpha = c m^2 \), where \( c \) is a positive constant.


\cite{libro}

\section{\textcolor{red}{Pratical application}}
\subsection{Preliminary analysis}
\subsection{Main results}

\section{\textcolor{red}{Comment and result}}


\begin{thebibliography}{99}

\bibitem{libro} Peter Müller, Fernando Andrés Quintana, Alejandro Jara, Tim Hanson, \textit{Bayesian Nonparametric Data Analysis}, Springer, 2015.

\bibitem{tesi} Riccardo Lazzarini, \textit{Bayesian Nonparametric Privacy-Preserving Synthetic Data Generation}, 2023.



\end{thebibliography}


\end{document}
